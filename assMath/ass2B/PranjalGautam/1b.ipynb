{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using normal pdf\n",
    "We import numpy and make our own database with some noise added over the output y.\\\n",
    "We create a Gradient Descent Linear Regression method to evaluate w so the the probability\\\n",
    " p(y|x)=N(y|wx,sigma) is maximised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=list(10*i for i in range(100,200))\n",
    "list2=np.random.normal(1,0.05,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34901.07240177, 30118.91443421, 31491.41234285, 31690.38331079,\n",
       "       34253.41421925, 33385.29664004, 34707.07559067, 34053.30850889,\n",
       "       34902.24383758, 36462.5012122 , 37020.44572398, 37818.08327282,\n",
       "       34144.38539272, 34264.24757919, 36864.94160063, 34875.22029673,\n",
       "       36700.37027523, 37519.5969311 , 38653.72055674, 37814.61465512,\n",
       "       37514.70197227, 41243.85104597, 38060.21640109, 41490.50665109,\n",
       "       41053.04961243, 37887.17468147, 39911.56663394, 45853.8134795 ,\n",
       "       41149.53039906, 38340.70920764, 42870.55438768, 41168.6732373 ,\n",
       "       43058.01852184, 40974.20406715, 39268.28226372, 44554.57316448,\n",
       "       45079.02089404, 41935.35306073, 43395.14253637, 43040.64338161,\n",
       "       47072.81832794, 46109.84451449, 48638.88836906, 44290.99131704,\n",
       "       45497.52089965, 46066.07299841, 47654.45416652, 48436.6541286 ,\n",
       "       46853.75587078, 49203.95177507, 47030.47152383, 45655.59873335,\n",
       "       51752.46614415, 42581.35585595, 50111.05424955, 52480.30932879,\n",
       "       46285.02043581, 50857.35033252, 50651.91137853, 57696.67653299,\n",
       "       51625.78997457, 50512.46573226, 59360.20067572, 46885.10288279,\n",
       "       52840.39290167, 49989.86218922, 52293.29539204, 56343.45775238,\n",
       "       58432.70542458, 56081.47926556, 55107.78764442, 50600.25058433,\n",
       "       52006.56136856, 52520.55990405, 54784.92438157, 52250.0745507 ,\n",
       "       52312.75126439, 53602.65561777, 60061.00749082, 54217.62586864,\n",
       "       56986.48772804, 59938.30938431, 61040.4035396 , 57037.66745249,\n",
       "       59561.59833724, 64234.90385416, 56435.46046362, 57306.37407589,\n",
       "       64362.33729366, 58347.39427375, 56459.85678489, 59125.00816149,\n",
       "       57010.45206312, 61494.80296834, 70477.08379065, 62593.57759395,\n",
       "       70420.93751967, 64391.85347377, 65922.17818568, 61719.94047161])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(100,200): # true slope theta=32\n",
    "    list2[i-100]=32*(10*i)*list2[i-100] #the dataset is values from 1000 to 1990 seperated by 10\n",
    "list2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The gradient descent function\n",
    "We take the loss function as the derivative of the negative log(pdf) which comes out to be:\\\n",
    "grad_w=-sum((yi-wxi)xi/sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,w,sigma,rate):\n",
    "    grad_w=0\n",
    "    for i in range(len(x)):\n",
    "        grad_w+=-(y[i]-w*x[i])*x[i]/sigma\n",
    "    return w-(grad_w*(rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=1\n",
    "rate=10**(-8)\n",
    "sigma=10\n",
    "epochs=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    w=gradient_descent(list1,list2,w,sigma,rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see we are getting a satisfactory value for w considering the added noise\\\n",
    "One main thing to NOTE: is sigma does NOT affect our algorithm in a unique way and should not be a parameter, instead all it does is that it changes the learning rate value due to our loss function having it in the denominator independent of the dataset. It is just better to not include sigma and assume it as one and use the learning rate to affect how our algorithm works instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.04849893796961"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
